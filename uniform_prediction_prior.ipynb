{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR dataset\n",
        "\n",
        "Set seeds, import data, define some functions"
      ],
      "metadata": {
        "id": "s3aSnFIjlwui"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUJsZ07wlCiG",
        "outputId": "3739d532-7d30-4c7d-9ab1-d2a9889dccc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "import warnings\n",
        "\n",
        "seed = 4398\n",
        "\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "cifar = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "(x_train_cifar, y_train_cifar), (x_test_cifar, y_test_cifar) = cifar\n",
        "input_shape_cifar = (32, 32, 3)\n",
        "num_classes = 10\n",
        "\n",
        "x_train_cifar = x_train_cifar.astype(\"float32\") / 255\n",
        "x_test_cifar = x_test_cifar.astype(\"float32\") / 255\n",
        "x_train_cifar = np.expand_dims(x_train_cifar, -1)\n",
        "x_test_cifar = np.expand_dims(x_test_cifar, -1)\n",
        "\n",
        "y_train_cifar = keras.utils.to_categorical(y_train_cifar, num_classes)\n",
        "y_test_cifar = keras.utils.to_categorical(y_test_cifar, num_classes)\n",
        "\n",
        "def create_cifar_model(activation=\"softmax\", input_shape=(32, 32, 3)):\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=input_shape))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "  model.add(layers.Dense(10, activation=activation))\n",
        "\n",
        "  return model\n",
        "\n",
        "def build_and_evaluate_model(activation, loss, epochs=5, verbose=False, batch_size=128):\n",
        "  model = create_cifar_model(activation=activation)\n",
        "\n",
        "  model.compile(loss=loss, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "  model.fit(x_train_cifar, y_train_cifar, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "  test_preds = model.predict(x_test_cifar, verbose=1)\n",
        "\n",
        "  test_scores = model.evaluate(x_test_cifar, y_test_cifar, verbose=1)\n",
        "  if verbose:\n",
        "    model.summary()\n",
        "    print(\"Test loss:\", test_scores[0])\n",
        "    print(\"Test accuracy:\", test_scores[1])\n",
        "\n",
        "  return model, test_scores, test_preds\n",
        "\n",
        "def within_epsilon(expected, value, epsilon=0.0001):\n",
        "  return abs(test['expected'] - loss) <= epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normal cross entropy code\n",
        "\n",
        "Copying cross entropy code from keras so I can replicate it and (later) change it"
      ],
      "metadata": {
        "id": "mhikcWxFpgBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_logits(output, from_logits, op_type, fn_name):\n",
        "    output_ = output\n",
        "    from_logits_ = from_logits\n",
        "\n",
        "    has_keras_logits = hasattr(output, \"_keras_logits\")\n",
        "    if has_keras_logits:\n",
        "        output_ = output._keras_logits\n",
        "        from_logits_ = True\n",
        "\n",
        "    from_expected_op_type = (\n",
        "        not isinstance(output, (tf.__internal__.EagerTensor, tf.Variable))\n",
        "        and output.op.type == op_type\n",
        "    ) and not has_keras_logits\n",
        "\n",
        "    if from_expected_op_type:\n",
        "        # When softmax activation function is used for output operation, we\n",
        "        # use logits from the softmax function directly to compute loss in order\n",
        "        # to prevent collapsing zero when training.\n",
        "        # See b/117284466\n",
        "        assert len(output.op.inputs) == 1\n",
        "        output_ = output.op.inputs[0]\n",
        "        from_logits_ = True\n",
        "\n",
        "    if from_logits and (has_keras_logits or from_expected_op_type):\n",
        "        warnings.warn(\n",
        "            f'\"`{fn_name}` received `from_logits=True`, but '\n",
        "            f\"the `output` argument was produced by a {op_type} \"\n",
        "            \"activation and thus does not represent logits. \"\n",
        "            \"Was this intended?\",\n",
        "            stacklevel=2,\n",
        "        )\n",
        "\n",
        "    return output_, from_logits_\n",
        "\n",
        "def backend_ce(target, output, from_logits=False, axis=-1):\n",
        "    target = tf.convert_to_tensor(target)\n",
        "    output = tf.convert_to_tensor(output)\n",
        "    target.shape.assert_is_compatible_with(output.shape)\n",
        "\n",
        "    output, from_logits = _get_logits(\n",
        "        output, from_logits, \"Softmax\", \"categorical_crossentropy\"\n",
        "    )\n",
        "    if from_logits:\n",
        "        return tf.nn.softmax_cross_entropy_with_logits(\n",
        "            labels=target, logits=output, axis=axis\n",
        "        )\n",
        "\n",
        "    # scale preds so that the class probas of each sample sum to 1\n",
        "    output = output / tf.reduce_sum(output, axis, True)\n",
        "    # Compute cross entropy from probabilities.\n",
        "    epsilon_ = keras.backend._constant_to_tensor(keras.backend.epsilon(), output.dtype.base_dtype)\n",
        "    output = tf.clip_by_value(output, epsilon_, 1.0 - epsilon_)\n",
        "    return -tf.reduce_sum(target * tf.math.log(output), axis)\n",
        "\n",
        "def keras_categorical_crossentropy(\n",
        "    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n",
        "):\n",
        "  y_pred = tf.convert_to_tensor(y_pred)\n",
        "  y_true = tf.cast(y_true, y_pred.dtype)\n",
        "  label_smoothing = tf.convert_to_tensor(label_smoothing, dtype=y_pred.dtype)\n",
        "\n",
        "  def _smooth_labels():\n",
        "      num_classes = tf.cast(tf.shape(y_true)[-1], y_pred.dtype)\n",
        "      return y_true * (1.0 - label_smoothing) + (\n",
        "          label_smoothing / num_classes\n",
        "      )\n",
        "\n",
        "  y_true = tf.__internal__.smart_cond.smart_cond(\n",
        "      label_smoothing, _smooth_labels, lambda: y_true\n",
        "  )\n",
        "\n",
        "  return backend_ce(\n",
        "      y_true, y_pred, from_logits=from_logits, axis=axis\n",
        "  )"
      ],
      "metadata": {
        "id": "AOSt3is5pev0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uniform prediction prior \n",
        "\n",
        "Update cross entropy code to prefer equal predictions on negative classes"
      ],
      "metadata": {
        "id": "ErxWKzMHp1BV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backend_ce_custom(target, output, from_logits=False, axis=-1, custom_version=1):\n",
        "    target = tf.convert_to_tensor(target)\n",
        "    output = tf.convert_to_tensor(output)\n",
        "    target.shape.assert_is_compatible_with(output.shape)\n",
        "\n",
        "    output, from_logits = _get_logits(\n",
        "        output, from_logits, \"Softmax\", \"categorical_crossentropy\"\n",
        "    )\n",
        "    #if from_logits:\n",
        "    #    return tf.nn.softmax_cross_entropy_with_logits(\n",
        "    #        labels=target, logits=output, axis=axis\n",
        "    #    )\n",
        "\n",
        "    # scale preds so that the class probas of each sample sum to 1\n",
        "    output = output / tf.reduce_sum(output, axis, True)\n",
        "    # Compute cross entropy from probabilities.\n",
        "    epsilon_ = keras.backend._constant_to_tensor(keras.backend.epsilon(), output.dtype.base_dtype)\n",
        "    output = tf.clip_by_value(output, epsilon_, 1.0 - epsilon_)\n",
        "\n",
        "    # Custom code here\n",
        "    inverse_mask = tf.ones_like(target) - target\n",
        "    pred_of_positive = tf.reduce_sum(tf.math.multiply(target, output))\n",
        "    remainder = tf.constant(1.) - pred_of_positive\n",
        "    # Uniform prediction prior among the negative classes\n",
        "    expected = remainder / tf.cast((len(target) - 1), tf.float32)\n",
        "\n",
        "    if custom_version == 1:\n",
        "      log_losses_negative = tf.reduce_sum(tf.math.multiply(inverse_mask, tf.math.abs(output - expected)))\n",
        "    elif custom_version == 2:\n",
        "      log_losses_negative = -tf.reduce_sum(tf.math.multiply(inverse_mask, tf.math.log(1 - tf.math.abs(output - expected))))\n",
        "    elif custom_version == 3:\n",
        "      log_losses_negative = tf.reduce_sum(tf.math.multiply(inverse_mask, 10*tf.math.abs(output - expected)))\n",
        "\n",
        "    log_losses_positive = -tf.reduce_sum(target * tf.math.log(output), axis)\n",
        "\n",
        "    return tf.math.add(log_losses_positive, log_losses_negative)\n",
        "\n",
        "def keras_categorical_crossentropy_custom_abs(\n",
        "    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n",
        "):\n",
        "  y_pred = tf.convert_to_tensor(y_pred)\n",
        "  y_true = tf.cast(y_true, y_pred.dtype)\n",
        "  label_smoothing = tf.convert_to_tensor(label_smoothing, dtype=y_pred.dtype)\n",
        "\n",
        "  def _smooth_labels():\n",
        "      num_classes = tf.cast(tf.shape(y_true)[-1], y_pred.dtype)\n",
        "      return y_true * (1.0 - label_smoothing) + (\n",
        "          label_smoothing / num_classes\n",
        "      )\n",
        "\n",
        "  y_true = tf.__internal__.smart_cond.smart_cond(\n",
        "      label_smoothing, _smooth_labels, lambda: y_true\n",
        "  )\n",
        "\n",
        "  return backend_ce_custom(\n",
        "      y_true, y_pred, from_logits=from_logits, axis=axis\n",
        "  )\n",
        "\n",
        "def keras_categorical_crossentropy_custom_log(\n",
        "    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n",
        "):\n",
        "  y_pred = tf.convert_to_tensor(y_pred)\n",
        "  y_true = tf.cast(y_true, y_pred.dtype)\n",
        "  label_smoothing = tf.convert_to_tensor(label_smoothing, dtype=y_pred.dtype)\n",
        "\n",
        "  def _smooth_labels():\n",
        "      num_classes = tf.cast(tf.shape(y_true)[-1], y_pred.dtype)\n",
        "      return y_true * (1.0 - label_smoothing) + (\n",
        "          label_smoothing / num_classes\n",
        "      )\n",
        "\n",
        "  y_true = tf.__internal__.smart_cond.smart_cond(\n",
        "      label_smoothing, _smooth_labels, lambda: y_true\n",
        "  )\n",
        "\n",
        "  return backend_ce_custom(\n",
        "      y_true, y_pred, from_logits=from_logits, axis=axis, custom_version=2\n",
        "  )\n",
        "\n",
        "\n",
        "def keras_categorical_crossentropy_custom_dramatic(\n",
        "    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n",
        "):\n",
        "  y_pred = tf.convert_to_tensor(y_pred)\n",
        "  y_true = tf.cast(y_true, y_pred.dtype)\n",
        "  label_smoothing = tf.convert_to_tensor(label_smoothing, dtype=y_pred.dtype)\n",
        "\n",
        "  def _smooth_labels():\n",
        "      num_classes = tf.cast(tf.shape(y_true)[-1], y_pred.dtype)\n",
        "      return y_true * (1.0 - label_smoothing) + (\n",
        "          label_smoothing / num_classes\n",
        "      )\n",
        "\n",
        "  y_true = tf.__internal__.smart_cond.smart_cond(\n",
        "      label_smoothing, _smooth_labels, lambda: y_true\n",
        "  )\n",
        "\n",
        "  return backend_ce_custom(\n",
        "      y_true, y_pred, from_logits=from_logits, axis=axis, custom_version=3\n",
        "  )"
      ],
      "metadata": {
        "id": "i_j_3ch-p1NT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of using a custom loss function\n",
        "\n",
        "keras_categorical_crossentropy_custom_abs([0, 0, 1], [0.3, 0.1, 0.6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkIi4rbrNWwM",
        "outputId": "0f0c5904-d327-49cd-e294-e893ecb79c15"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.71082556>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_tests_log = [\n",
        "    {\n",
        "        'expected': -np.log(0.6),\n",
        "        'y_true': [0, 0, 1],\n",
        "        'y_pred': [0.2, 0.2, 0.6]\n",
        "    },\n",
        "    {\n",
        "        'expected': -(np.log(0.6) + np.log(0.9) + np.log(0.9)),\n",
        "        'y_true': [0, 0, 1],\n",
        "        'y_pred': [0.3, 0.1, 0.6]\n",
        "    },\n",
        "    {\n",
        "        'expected': -(np.log(0.6) + np.log(0.9) + np.log(0.95) + np.log(0.95)),\n",
        "        'y_true': [0, 0, 1, 0, 0],\n",
        "        'y_pred': [0.2, 0.05, 0.6, 0.05, 0.1]\n",
        "    },\n",
        "]\n",
        "\n",
        "for i, test in enumerate(loss_tests_log):\n",
        "  loss = keras_categorical_crossentropy_custom_log(test['y_true'], test['y_pred'])\n",
        "  if not within_epsilon(test['expected'], loss):\n",
        "    print(f\"Failed test {i} (log), expected {test['expected']}, got {loss}\")\n",
        "\n",
        "loss_tests_abs = [\n",
        "    {\n",
        "        'expected': -np.log(0.6),\n",
        "        'y_true': [0, 0, 1],\n",
        "        'y_pred': [0.2, 0.2, 0.6]\n",
        "    },\n",
        "    {\n",
        "        'expected': -np.log(0.6) + 0.1 + 0.1,\n",
        "        'y_true': [0, 0, 1],\n",
        "        'y_pred': [0.3, 0.1, 0.6]\n",
        "    },\n",
        "    {\n",
        "        'expected': -np.log(0.6) + 0.1 + 0.05 + 0.05,\n",
        "        'y_true': [0, 0, 1, 0, 0],\n",
        "        'y_pred': [0.2, 0.05, 0.6, 0.05, 0.1]\n",
        "    },\n",
        "]\n",
        "\n",
        "for i, test in enumerate(loss_tests_abs):\n",
        "  loss = keras_categorical_crossentropy_custom_abs(test['y_true'], test['y_pred'])\n",
        "  if not within_epsilon(test['expected'], loss):\n",
        "    print(f\"Failed test {i} (abs), expected {test['expected']}, got {loss}\")\n",
        "\n",
        "print(\"Tests finished\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcExFHHQRPjB",
        "outputId": "4ad6bf0e-ee2d-4284-ae08-74b3afd720dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tests finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run experiments"
      ],
      "metadata": {
        "id": "OewWCAOaZXiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "specifications = [\n",
        "    {\n",
        "        'name': 'Traditional',\n",
        "        'loss': \"categorical_crossentropy\",\n",
        "        'epochs': epochs,\n",
        "        'activation': 'softmax',\n",
        "    },\n",
        "    {\n",
        "        'name': 'Traditional with BCE',\n",
        "        'loss': \"binary_crossentropy\",\n",
        "        'epochs': epochs,\n",
        "        'activation': 'sigmoid',\n",
        "    },\n",
        "    {\n",
        "        'name': 'Replicated CE',\n",
        "        'loss': keras_categorical_crossentropy,\n",
        "        'epochs': epochs,\n",
        "        'activation': 'softmax',\n",
        "    },\n",
        "    {\n",
        "        'name': 'CE with abs uniform prediction prior',\n",
        "        'loss': keras_categorical_crossentropy_custom_abs,\n",
        "        'epochs': epochs,\n",
        "        'activation': 'softmax',\n",
        "    },\n",
        "    {\n",
        "        'name': 'CE with log uniform prediction prior',\n",
        "        'loss': keras_categorical_crossentropy_custom_log,\n",
        "        'epochs': epochs,\n",
        "        'activation': 'softmax',\n",
        "    },\n",
        "    {\n",
        "        'name': 'CE with strong uniform prediction prior',\n",
        "        'loss': keras_categorical_crossentropy_custom_dramatic,\n",
        "        'epochs': epochs,\n",
        "        'activation': 'softmax',\n",
        "    },\n",
        "]\n",
        "\n",
        "results = {}\n",
        "\n",
        "for experiment in specifications:\n",
        "   print(f\"Training model \\\"{experiment['name']}\\\"\")\n",
        "   (model, test_scores, test_preds) = build_and_evaluate_model(\n",
        "       experiment['activation'], experiment['loss'], experiment['epochs'])\n",
        "   results[experiment['name']] = {'model': model, 'test_scores': test_scores, 'test_preds': test_preds}\n",
        "\n",
        "for experiment in results:\n",
        "  test_accuracy = results[experiment]['test_scores'][1]\n",
        "  print(f\"{experiment}: test accuracy of {test_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTwwc2H7mKLu",
        "outputId": "af52a43d-508e-4e12-b1ec-6db6bc8b7dc5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model \"Traditional\"\n",
            "Epoch 1/10\n",
            "352/352 [==============================] - 61s 169ms/step - loss: 1.5517 - accuracy: 0.4446 - val_loss: 1.2687 - val_accuracy: 0.5522\n",
            "Epoch 2/10\n",
            "352/352 [==============================] - 59s 168ms/step - loss: 1.1829 - accuracy: 0.5847 - val_loss: 1.1026 - val_accuracy: 0.6172\n",
            "Epoch 3/10\n",
            "352/352 [==============================] - 59s 167ms/step - loss: 1.0291 - accuracy: 0.6387 - val_loss: 1.0249 - val_accuracy: 0.6372\n",
            "Epoch 4/10\n",
            "352/352 [==============================] - 60s 170ms/step - loss: 0.9412 - accuracy: 0.6719 - val_loss: 0.9757 - val_accuracy: 0.6590\n",
            "Epoch 5/10\n",
            "352/352 [==============================] - 59s 167ms/step - loss: 0.8703 - accuracy: 0.6973 - val_loss: 0.9130 - val_accuracy: 0.6864\n",
            "Epoch 6/10\n",
            "352/352 [==============================] - 59s 167ms/step - loss: 0.8135 - accuracy: 0.7168 - val_loss: 0.8732 - val_accuracy: 0.7002\n",
            "Epoch 7/10\n",
            "352/352 [==============================] - 60s 169ms/step - loss: 0.7640 - accuracy: 0.7332 - val_loss: 0.8992 - val_accuracy: 0.6954\n",
            "Epoch 8/10\n",
            "352/352 [==============================] - 57s 163ms/step - loss: 0.7160 - accuracy: 0.7504 - val_loss: 0.8773 - val_accuracy: 0.7024\n",
            "Epoch 9/10\n",
            "352/352 [==============================] - 59s 167ms/step - loss: 0.6708 - accuracy: 0.7662 - val_loss: 0.8729 - val_accuracy: 0.7044\n",
            "Epoch 10/10\n",
            "352/352 [==============================] - 60s 169ms/step - loss: 0.6398 - accuracy: 0.7777 - val_loss: 0.8789 - val_accuracy: 0.7066\n",
            "313/313 [==============================] - 6s 18ms/step\n",
            "313/313 [==============================] - 6s 18ms/step - loss: 0.9077 - accuracy: 0.6963\n",
            "Training model \"Traditional with BCE\"\n",
            "Epoch 1/10\n",
            "352/352 [==============================] - 60s 167ms/step - loss: 0.2552 - accuracy: 0.4116 - val_loss: 0.2095 - val_accuracy: 0.5356\n",
            "Epoch 2/10\n",
            "352/352 [==============================] - 59s 167ms/step - loss: 0.2000 - accuracy: 0.5626 - val_loss: 0.1867 - val_accuracy: 0.6012\n",
            "Epoch 3/10\n",
            "352/352 [==============================] - 59s 168ms/step - loss: 0.1769 - accuracy: 0.6215 - val_loss: 0.1737 - val_accuracy: 0.6284\n",
            "Epoch 4/10\n",
            "352/352 [==============================] - 57s 162ms/step - loss: 0.1627 - accuracy: 0.6573 - val_loss: 0.1614 - val_accuracy: 0.6574\n",
            "Epoch 5/10\n",
            "352/352 [==============================] - 58s 165ms/step - loss: 0.1523 - accuracy: 0.6824 - val_loss: 0.1539 - val_accuracy: 0.6820\n",
            "Epoch 6/10\n",
            "352/352 [==============================] - 58s 164ms/step - loss: 0.1428 - accuracy: 0.7038 - val_loss: 0.1500 - val_accuracy: 0.6888\n",
            "Epoch 7/10\n",
            "352/352 [==============================] - 58s 166ms/step - loss: 0.1361 - accuracy: 0.7224 - val_loss: 0.1481 - val_accuracy: 0.6942\n",
            "Epoch 8/10\n",
            "352/352 [==============================] - 62s 175ms/step - loss: 0.1290 - accuracy: 0.7373 - val_loss: 0.1541 - val_accuracy: 0.6846\n",
            "Epoch 9/10\n",
            "352/352 [==============================] - 59s 168ms/step - loss: 0.1229 - accuracy: 0.7511 - val_loss: 0.1405 - val_accuracy: 0.7132\n",
            "Epoch 10/10\n",
            "352/352 [==============================] - 58s 165ms/step - loss: 0.1178 - accuracy: 0.7663 - val_loss: 0.1430 - val_accuracy: 0.7112\n",
            "313/313 [==============================] - 4s 14ms/step\n",
            "313/313 [==============================] - 6s 20ms/step - loss: 0.1480 - accuracy: 0.6987\n",
            "Training model \"Replicated CE\"\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "352/352 [==============================] - 58s 162ms/step - loss: 1.6185 - accuracy: 0.4214 - val_loss: 1.3104 - val_accuracy: 0.5416\n",
            "Epoch 2/10\n",
            "352/352 [==============================] - 58s 164ms/step - loss: 1.2178 - accuracy: 0.5672 - val_loss: 1.1628 - val_accuracy: 0.6000\n",
            "Epoch 3/10\n",
            "352/352 [==============================] - 58s 164ms/step - loss: 1.0476 - accuracy: 0.6318 - val_loss: 1.0229 - val_accuracy: 0.6366\n",
            "Epoch 4/10\n",
            "352/352 [==============================] - 58s 165ms/step - loss: 0.9534 - accuracy: 0.6660 - val_loss: 0.9626 - val_accuracy: 0.6598\n",
            "Epoch 5/10\n",
            "352/352 [==============================] - 60s 170ms/step - loss: 0.8810 - accuracy: 0.6920 - val_loss: 0.9102 - val_accuracy: 0.6864\n",
            "Epoch 6/10\n",
            "352/352 [==============================] - 57s 163ms/step - loss: 0.8185 - accuracy: 0.7134 - val_loss: 0.9027 - val_accuracy: 0.6934\n",
            "Epoch 7/10\n",
            "352/352 [==============================] - 61s 174ms/step - loss: 0.7768 - accuracy: 0.7295 - val_loss: 0.8785 - val_accuracy: 0.7030\n",
            "Epoch 8/10\n",
            "352/352 [==============================] - 59s 167ms/step - loss: 0.7286 - accuracy: 0.7472 - val_loss: 0.9141 - val_accuracy: 0.6896\n",
            "Epoch 9/10\n",
            "352/352 [==============================] - 59s 167ms/step - loss: 0.6818 - accuracy: 0.7632 - val_loss: 0.8414 - val_accuracy: 0.7164\n",
            "Epoch 10/10\n",
            "352/352 [==============================] - 59s 168ms/step - loss: 0.6479 - accuracy: 0.7740 - val_loss: 0.8568 - val_accuracy: 0.7160\n",
            "313/313 [==============================] - 5s 16ms/step\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 0.8881 - accuracy: 0.7041\n",
            "Training model \"CE with abs uniform prediction prior\"\n",
            "Epoch 1/10\n",
            "352/352 [==============================] - 61s 167ms/step - loss: 167.0446 - accuracy: 0.3040 - val_loss: 150.3251 - val_accuracy: 0.3620\n",
            "Epoch 2/10\n",
            "352/352 [==============================] - 59s 168ms/step - loss: 156.4915 - accuracy: 0.3047 - val_loss: 173.5329 - val_accuracy: 0.2192\n",
            "Epoch 3/10\n",
            "352/352 [==============================] - 60s 169ms/step - loss: 156.3730 - accuracy: 0.2457 - val_loss: 166.5884 - val_accuracy: 0.1744\n",
            "Epoch 4/10\n",
            "352/352 [==============================] - 59s 167ms/step - loss: 149.7939 - accuracy: 0.2422 - val_loss: 147.0380 - val_accuracy: 0.2286\n",
            "Epoch 5/10\n",
            "352/352 [==============================] - 59s 168ms/step - loss: 144.7951 - accuracy: 0.2632 - val_loss: 143.0599 - val_accuracy: 0.2406\n",
            "Epoch 6/10\n",
            "352/352 [==============================] - 58s 165ms/step - loss: 144.4075 - accuracy: 0.2490 - val_loss: 141.8073 - val_accuracy: 0.2150\n",
            "Epoch 7/10\n",
            "352/352 [==============================] - 57s 163ms/step - loss: 142.1530 - accuracy: 0.2523 - val_loss: 144.4490 - val_accuracy: 0.2542\n",
            "Epoch 8/10\n",
            "352/352 [==============================] - 57s 161ms/step - loss: 145.7618 - accuracy: 0.2236 - val_loss: 139.2571 - val_accuracy: 0.2336\n",
            "Epoch 9/10\n",
            "352/352 [==============================] - 59s 167ms/step - loss: 142.0117 - accuracy: 0.2390 - val_loss: 141.2525 - val_accuracy: 0.1754\n",
            "Epoch 10/10\n",
            "352/352 [==============================] - 60s 172ms/step - loss: 142.2194 - accuracy: 0.2248 - val_loss: 148.2618 - val_accuracy: 0.2208\n",
            "313/313 [==============================] - 6s 18ms/step\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 34.9007 - accuracy: 0.2077\n",
            "Training model \"CE with log uniform prediction prior\"\n",
            "Epoch 1/10\n",
            "352/352 [==============================] - 61s 169ms/step - loss: 190.1783 - accuracy: 0.0182 - val_loss: 170.3785 - val_accuracy: 0.0062\n",
            "Epoch 2/10\n",
            "352/352 [==============================] - 58s 165ms/step - loss: 174.7785 - accuracy: 0.0101 - val_loss: 165.8712 - val_accuracy: 0.0052\n",
            "Epoch 3/10\n",
            "352/352 [==============================] - 58s 164ms/step - loss: 165.9471 - accuracy: 0.0057 - val_loss: 163.6799 - val_accuracy: 0.0052\n",
            "Epoch 4/10\n",
            "352/352 [==============================] - 59s 166ms/step - loss: 165.4578 - accuracy: 0.0054 - val_loss: 166.1423 - val_accuracy: 0.0064\n",
            "Epoch 5/10\n",
            "352/352 [==============================] - 59s 168ms/step - loss: 163.4197 - accuracy: 0.0044 - val_loss: 161.6268 - val_accuracy: 0.0046\n",
            "Epoch 6/10\n",
            "352/352 [==============================] - 57s 163ms/step - loss: 162.9143 - accuracy: 0.0046 - val_loss: 159.1538 - val_accuracy: 0.0038\n",
            "Epoch 7/10\n",
            "352/352 [==============================] - 61s 174ms/step - loss: 167.6008 - accuracy: 0.0072 - val_loss: 172.0403 - val_accuracy: 0.0066\n",
            "Epoch 8/10\n",
            "352/352 [==============================] - 57s 162ms/step - loss: 164.6647 - accuracy: 0.0045 - val_loss: 158.0569 - val_accuracy: 0.0034\n",
            "Epoch 9/10\n",
            "352/352 [==============================] - 58s 166ms/step - loss: 159.8967 - accuracy: 0.0030 - val_loss: 162.4288 - val_accuracy: 0.0038\n",
            "Epoch 10/10\n",
            "352/352 [==============================] - 59s 166ms/step - loss: 157.8974 - accuracy: 0.0030 - val_loss: 156.4772 - val_accuracy: 0.0028\n",
            "313/313 [==============================] - 4s 14ms/step\n",
            "313/313 [==============================] - 6s 18ms/step - loss: 38.7023 - accuracy: 0.0033\n",
            "Training model \"CE with strong uniform prediction prior\"\n",
            "Epoch 1/10\n",
            "352/352 [==============================] - 61s 168ms/step - loss: 1641.3708 - accuracy: 0.3026 - val_loss: 1374.8514 - val_accuracy: 0.4048\n",
            "Epoch 2/10\n",
            "352/352 [==============================] - 59s 167ms/step - loss: 1358.7179 - accuracy: 0.3895 - val_loss: 1325.2737 - val_accuracy: 0.3908\n",
            "Epoch 3/10\n",
            "352/352 [==============================] - 59s 169ms/step - loss: 1322.3582 - accuracy: 0.4073 - val_loss: 1376.0083 - val_accuracy: 0.3882\n",
            "Epoch 4/10\n",
            "352/352 [==============================] - 59s 168ms/step - loss: 1304.0745 - accuracy: 0.4286 - val_loss: 1286.5009 - val_accuracy: 0.4494\n",
            "Epoch 5/10\n",
            "352/352 [==============================] - 58s 166ms/step - loss: 1297.0490 - accuracy: 0.4331 - val_loss: 1294.0048 - val_accuracy: 0.4238\n",
            "Epoch 6/10\n",
            "352/352 [==============================] - 60s 169ms/step - loss: 1284.3292 - accuracy: 0.4458 - val_loss: 1315.4681 - val_accuracy: 0.4450\n",
            "Epoch 7/10\n",
            "352/352 [==============================] - 59s 167ms/step - loss: 1288.6359 - accuracy: 0.4141 - val_loss: 1292.1313 - val_accuracy: 0.3970\n",
            "Epoch 8/10\n",
            "352/352 [==============================] - 59s 168ms/step - loss: 1272.5533 - accuracy: 0.4599 - val_loss: 1274.7780 - val_accuracy: 0.4678\n",
            "Epoch 9/10\n",
            "352/352 [==============================] - 59s 168ms/step - loss: 1272.5817 - accuracy: 0.4342 - val_loss: 1279.3320 - val_accuracy: 0.4366\n",
            "Epoch 10/10\n",
            "352/352 [==============================] - 59s 167ms/step - loss: 1262.9313 - accuracy: 0.4742 - val_loss: 1270.3763 - val_accuracy: 0.4564\n",
            "313/313 [==============================] - 5s 14ms/step\n",
            "313/313 [==============================] - 6s 18ms/step - loss: 321.5640 - accuracy: 0.4545\n",
            "Traditional: test accuracy of 0.6963000297546387\n",
            "Traditional with BCE: test accuracy of 0.6987000107765198\n",
            "Replicated CE: test accuracy of 0.7041000127792358\n",
            "CE with abs uniform prediction prior: test accuracy of 0.2076999992132187\n",
            "CE with log uniform prediction prior: test accuracy of 0.0032999999821186066\n",
            "CE with strong uniform prediction prior: test accuracy of 0.4544999897480011\n",
            "CPU times: user 1h 30min 15s, sys: 1min 1s, total: 1h 31min 16s\n",
            "Wall time: 1h 2min 22s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output some statistics about the various predictions"
      ],
      "metadata": {
        "id": "k6TJ7qBtZgzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "all_incorrect_base = []\n",
        "all_incorrect_custom = []\n",
        "\n",
        "std_of_wrongs_base = []\n",
        "std_of_wrongs_custom = []\n",
        "\n",
        "preds_cifar_base = results['Replicated CE']['test_preds']\n",
        "preds_cifar_custom = results['CE with strong uniform prediction prior']['test_preds']\n",
        "\n",
        "for i in range(len(y_test_cifar)):\n",
        "  local_incorrect_base = []\n",
        "  local_incorrect_custom = []\n",
        "  for j in range(len(preds_cifar_base[i])):\n",
        "    if y_test_cifar[i][j] != 1:\n",
        "      all_incorrect_base.append(preds_cifar_base[i][j])\n",
        "      all_incorrect_custom.append(preds_cifar_custom[i][j])\n",
        "      local_incorrect_base.append(preds_cifar_base[i][j])\n",
        "      local_incorrect_custom.append(preds_cifar_custom[i][j])\n",
        "\n",
        "  std_of_wrongs_base.append(np.std(local_incorrect_base))\n",
        "  std_of_wrongs_custom.append(np.std(local_incorrect_custom))\n",
        "\n",
        "print(f\"Mean prediction for incorrect classes (base model) {np.mean(all_incorrect_base)}\")\n",
        "print(f\"Mean prediction for incorrect classes (custom loss)) {np.mean(all_incorrect_custom)}\")\n",
        "\n",
        "print(f\"Std dev of predictions for incorrect classes (base model) {np.std(all_incorrect_base)}\")\n",
        "print(f\"Std dev of predictions for incorrect classes (custom loss)) {np.std(all_incorrect_custom)}\")\n",
        "\n",
        "print(f\"Mean of row-level std deviations of predictions for incorrect classes (base model) {np.mean(std_of_wrongs_base)}\")\n",
        "print(f\"Mean of row-level std deviations of predictions for incorrect classes (custom loss) {np.mean(std_of_wrongs_custom)}\")\n",
        "\n",
        "plt.hist(all_incorrect_base, bins=20)\n",
        "plt.show()\n",
        "\n",
        "plt.hist(all_incorrect_custom, bins=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "zfd751xaC4g_",
        "outputId": "00a88988-b05b-4aaa-aaa0-6ffa85043540"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean prediction for incorrect classes (base model) 0.0412888266146183\n",
            "Mean prediction for incorrect classes (custom loss)) 0.09487645328044891\n",
            "Std dev of predictions for incorrect classes (base model) 0.12280698865652084\n",
            "Std dev of predictions for incorrect classes (custom loss)) 0.05341934412717819\n",
            "Mean of row-level std deviations of predictions for incorrect classes (base model) 0.0806240513920784\n",
            "Mean of row-level std deviations of predictions for incorrect classes (custom loss) 0.052581965923309326\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}